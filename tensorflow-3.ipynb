{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Classification with logistic function\n",
    "- https://ko.wikipedia.org/wiki/로지스틱_회귀\n",
    "- https://www.youtube.com/watch?v=6vzchGYEJBc&list=PLlMkM4tgfjnLSOjrEJN31gZATbcj_MpUm&index=11&spfreload=1\n",
    "- sigmoid function: $ y = {1}/({1+\\exp({-x})})$\n",
    "- a linear regression or any other function can be fed into sigmoid to result in a value in the range of $[0,1]$.\n",
    "- binary classification\n",
    "- Logistic Hypothesis model: $ H(x) = 1 / (1 + \\exp(- W x ) )$\n",
    "- cost functon: $ cost(W) = \\frac{1}{m} \\sum_{i=1}^m (H(x^i) - y^i)^2 $. This is highly non-linear. No good for SGD optimization.\n",
    "- Instead, use $C(H(x), y|W) = -\\log(H(x))$ if $y=1$, else if $y=0$, $C(H(x),y) = -\\log(1-H(x))$. This results in the following cost function:\n",
    "\n",
    "$cost = \\frac{1}{m}\\sum C(H(x), y) = \\frac{1}{m}\\sum -y\\log(H(x)) - (1-y)\\log(1-H(x))$\n",
    "\n",
    "### cost function in tf\n",
    "`` cost = tf.reduce_mean(-tf.reduce_sum (Y*tf.log(H) + (1-Y)*tf.log(1-H)))``\n",
    "### minimize\n",
    "-``a=tf.Variable(0.1)``\n",
    "\n",
    "-``opt = tf.train.GradientDescentOptimizer(a)``\n",
    "\n",
    "-``train = opt.minimize(cost)``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/2001  1.96969 [[ 0.88889956 -0.48498845 -0.45460814]]\n",
      "100/2001  0.56253 [[-0.44403985 -0.16196413  0.4294897 ]]\n",
      "200/2001  0.447474 [[-1.49681008 -0.07871286  0.59690756]]\n",
      "300/2001  0.381553 [[-2.29517007 -0.00916002  0.71698141]]\n",
      "400/2001  0.339596 [[-2.93226719  0.04101916  0.81775784]]\n",
      "500/2001  0.31052 [[-3.46255231  0.07795615  0.90590245]]\n",
      "600/2001  0.289028 [[-3.91833472  0.1060717   0.98470694]]\n",
      "700/2001  0.272344 [[-4.31974936  0.12809558  1.05631089]]\n",
      "800/2001  0.258897 [[-4.67998981  0.14574523  1.12221396]]\n",
      "900/2001  0.247733 [[-5.00809193  0.16014707  1.18350482]]\n",
      "1000/2001  0.238243 [[-5.31048346  0.17207024  1.24099517]]\n",
      "1100/2001  0.23002 [[-5.59186983  0.18205783  1.29530132]]\n",
      "1200/2001  0.222781 [[-5.85580206  0.19050591  1.34690356]]\n",
      "1300/2001  0.216322 [[-6.10500956  0.19770892  1.39618087]]\n",
      "1400/2001  0.210496 [[-6.3416338   0.20389302  1.44343603]]\n",
      "1500/2001  0.20519 [[-6.56738329  0.20923269  1.48891592]]\n",
      "1600/2001  0.200319 [[-6.78364277  0.21386638  1.53282344]]\n",
      "1700/2001  0.195815 [[-6.99153996  0.21790388  1.57532704]]\n",
      "1800/2001  0.191625 [[-7.19201326  0.22143425  1.61656845]]\n",
      "1900/2001  0.187707 [[-7.38584566  0.22453137  1.65666628]]\n",
      "2000/2001  0.184025 [[-7.573699    0.22725552  1.69572294]]\n",
      "Finished Learning.\n"
     ]
    }
   ],
   "source": [
    "xy = np.loadtxt('train-logistic-r.txt', unpack=True, dtype='float32')\n",
    "xdata = xy[0:-1]\n",
    "ydata = xy[-1]\n",
    "\n",
    "X=tf.placeholder(tf.float32)\n",
    "Y=tf.placeholder(tf.float32)\n",
    "W=tf.Variable(tf.random_uniform([1,len(xdata)],-1.,1.))\n",
    "\n",
    "#\n",
    "h = tf.matmul(W , X)\n",
    "hypo = tf.div(1., 1.+tf.exp(-h))\n",
    "#cost\n",
    "cost = - tf.reduce_mean( Y*tf.log(hypo) + (1.-Y)*tf.log(1.-hypo) )\n",
    "\n",
    "#minimize\n",
    "a = tf.Variable(0.1) # learning rate\n",
    "optimizer = tf.train.GradientDescentOptimizer(a)\n",
    "trainer = optimizer.minimize(cost)\n",
    "\n",
    "#\n",
    "ss = tf.Session(); \n",
    "ss.run(tf.global_variables_initializer())\n",
    "\n",
    "# train!\n",
    "maxiter=2001\n",
    "for step in range(maxiter):\n",
    "    ss.run (trainer, feed_dict={X:xdata, Y:ydata})\n",
    "    if step%100 == 0:\n",
    "        print('{}/{} '.format(step, maxiter), \n",
    "             ss.run(cost, feed_dict={X:xdata, Y:ydata}),\n",
    "             ss.run(W))\n",
    "print ('Finished Learning.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test data: \n",
      "[[1 1 1 1]\n",
      " [2 5 4 3]\n",
      " [2 5 3 5]]\n",
      "test result probability:  [[ 0.02348239  0.88505471  0.1711487   0.83014882]]\n",
      "test result True/False:  [[False  True False  True]]\n"
     ]
    }
   ],
   "source": [
    "# run the classifier learned using some test input data\n",
    "# x1 = 4 시간 공부 x2 = 3번 수업참석 ...\n",
    "# Four test cases in total, \n",
    "xtest = np.array([[1,2,2],\n",
    "                  [1,5,5], \n",
    "                  [1, 4, 3], \n",
    "                  [1, 3, 5]]).transpose()\n",
    "print ('test data: ')\n",
    "print (xtest)\n",
    "\n",
    "# compute the predicted probability\n",
    "prob = ss.run(hypo, feed_dict={X: xtest}) # logistic function = cross entropy for binary!\n",
    "print ('test result probability: ', prob)\n",
    "print ('test result True/False: ', prob>0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
