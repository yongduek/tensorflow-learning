{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-class classification with Softmax regression\n",
    "\n",
    "- https://www.youtube.com/watch?v=MFAnsx1y9ZI&index=13&list=PLlMkM4tgfjnLSOjrEJN31gZATbcj_MpUm\n",
    "\n",
    "- multinomial classification = 3 개의 독립된 클래시파이어를 만들면 즉 각 클래서를 선택하는 클래시파이어를 만드는 것인데, 그냥 W 행렬을 1x3 짜리 3개를 만들지 말고 3x3 짜리 한 개를 만들자. 그러면, $Y= X * W$ 에서 얻어지는 $Y$는 3차원 벡터가 된다.\n",
    "- 그러면 $Y$ 값을 확률값으로 변경하고자하면 어떻게 해야하나. softmax 함수를 사용하면 된다.\n",
    "- $Softmax(y_i) = \\frac{\\exp{y_i}}{\\sum \\exp{y_k}}$\n",
    "- https://www.tensorflow.org/api_docs/python/tf/nn/softmax\n",
    "- ``log_softmax`` 가 정의되어 있다. https://www.tensorflow.org/versions/r0.10/api_docs/python/nn/classification\n",
    "- https://www.tensorflow.org/get_started/mnist/beginners\n",
    "- https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/mnist/mnist_softmax.py : may be numerically unstable so, ``tf.nn.softmax_cross_entropy_with_logits()`` 를 대신 사용한다.\n",
    "\n",
    "## 학습을 위한 코스트 함수는?\n",
    "- cross-entropy 를 사용하자\n",
    "- $D (S, L) = -\\sum_i L_i \\log(S_i)$, where $S$ is the prediction output vector from softmax and $L$ is the label vector (e.g. [0, 1, 0]).\n",
    "\n",
    "### Ref.\n",
    "- http://stackoverflow.com/questions/40675182/tensorflow-log-softmax-tf-nn-logtf-nn-softmaxpredict-tf-nn-softmax-cross-ent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xdata:  [[ 1.  2.  1.]\n",
      " [ 1.  3.  2.]\n",
      " [ 1.  3.  4.]\n",
      " [ 1.  5.  5.]\n",
      " [ 1.  7.  5.]\n",
      " [ 1.  2.  5.]\n",
      " [ 1.  6.  6.]\n",
      " [ 1.  7.  7.]]\n",
      "ydata:  [[ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]]\n",
      "notice that each row corresponds to one data input\n"
     ]
    }
   ],
   "source": [
    "xy = np.loadtxt('train-softmax.txt', unpack=True, dtype='float32')\n",
    "xdata = np.transpose(xy[0:3])\n",
    "ydata = np.transpose(xy[3:])\n",
    "print ('xdata: ', xdata)\n",
    "print ('ydata: ', ydata)\n",
    "print ('notice that each row corresponds to one data input')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-3-806eecf638f5>, line 13)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-3-806eecf638f5>\"\u001b[0;36m, line \u001b[0;32m13\u001b[0m\n\u001b[0;31m    ```\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# TF Graph Input\n",
    "X = tf.placeholder (\"float\", [None,3])\n",
    "Y = tf.placeholder (\"float\", [None,3])\n",
    "\n",
    "input_dim = 3\n",
    "output_dim = 3\n",
    "W = tf.Variable(tf.random_uniform([input_dim, output_dim], -1,1))\n",
    "hypothesis = tf.nn.softmax (tf.matmul(X, W))\n",
    "\n",
    "#cross entropy\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(Y*tf.log(hypothesis), axis=1))\n",
    "\n",
    "```\n",
    "cost = tf.reduce_mean( \n",
    "    tf.nn.softmax_cross_entropy_with_logits (labels=Y, logits=hypothesis))\n",
    "```\n",
    "learning_rate = 0.1\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "with tf.Session() as ss:\n",
    "    ss.run(tf.global_variables_initializer())\n",
    "    feed_dict = {X:xdata[:2], Y:ydata[:2]}\n",
    "    print ('W:', ss.run(W))\n",
    "    print ('X', ss.run(X, feed_dict = {X:xdata}))\n",
    "    print ('Y', ss.run(Y, feed_dict = {Y:ydata}))\n",
    "    print ('softmax(X*W)', ss.run(tf.nn.softmax(tf.matmul(X,W)), \n",
    "                                  feed_dict={X:xdata[:2]}))\n",
    "    print ('hypothesis=', ss.run(hypothesis, feed_dict))\n",
    "    print ('tf.log(hypothesis)=',\n",
    "          ss.run(\n",
    "          tf.log(hypothesis), feed_dict))\n",
    "    print ('Y*tf.log(hypothesis)=',\n",
    "          ss.run(Y*tf.log(hypothesis), feed_dict))\n",
    "    print ('tf.reduce_sum(Y*tf.log(hypothesis), axis=1)=',\n",
    "          ss.run(tf.reduce_sum(Y*tf.log(hypothesis), axis=1), feed_dict))\n",
    "    print ('cost = tf.reduce_mean(-tf.reduce_sum) = ', \n",
    "           ss.run(cost, feed_dict))\n",
    "\n",
    "    feed_dict = {X:xdata, Y:ydata}\n",
    "    \n",
    "    for step in range(20001):\n",
    "        ss.run(optimizer, feed_dict)\n",
    "        if step%2000==0:\n",
    "            print ('{}/2001 cost={} '.format(\n",
    "                step,\n",
    "                ss.run(cost, feed_dict))\n",
    "            )\n",
    "            \n",
    "    # see the learning result\n",
    "    print ('--- learning result ---')\n",
    "    re = ss.run(hypothesis, feed_dict)\n",
    "    rIndx = [ss.run(tf.arg_max(re,1))]\n",
    "    print ('hypothesis=', re)\n",
    "    print ('predicted Indx=', rIndx)\n",
    "    \n",
    "    testX = [ [1,11,7], [1,3,4], [1,1,0]]\n",
    "    print ('test output: ', ss.run(tf.arg_max(hypothesis,1), \n",
    "                                   feed_dict={X:testX}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 결론\n",
    "\n",
    "대략 돌아가는 상황을 파악하는 용도로만 사용하자. 데이터 8개로 피팅이 정말 잘 될 거라고 생각하지는 말아야하지않을까?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
